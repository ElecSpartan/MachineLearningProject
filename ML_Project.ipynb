{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project Description\n",
        "The data used in this project will help to identify whether a person is going to recover from \n",
        "coronavirus symptoms or not based on some pre-defined standard symptoms. These symptoms are \n",
        "based on guidelines given by the World Health Organization (WHO).\n",
        "This dataset has daily level information on the number of affected cases, deaths and recovery from \n",
        "2019 novel coronavirus. Please note that this is a time series data and so the number of cases on \n",
        "any given day is the cumulative number.\n",
        "The data is available from 22 Jan, 2020. Data is in “data.csv”.\n",
        "The dataset contains 14 major variables that will be having an impact on whether someone has \n",
        "recovered or not, the description of each variable are as follows,\n",
        "1. Country: where the person resides\n",
        "2. Location: which part in the Country\n",
        "3. Age: Classification of the age group for each person, based on WHO Age Group Standard\n",
        "4. Gender: Male or Female \n",
        "5. Visited_Wuhan: whether the person has visited Wuhan, China or not\n",
        "6. From_Wuhan: whether the person is from Wuhan, China or not\n",
        "7. Symptoms: there are six families of symptoms that are coded in six fields.\n",
        "13. Time_before_symptoms_appear: \n",
        "14. Result: death (1) or recovered (0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### First we will import the main libraries for the whole Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load and Visualize the Data... since it is of a high dimensional we will just see the table of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    location  country  gender   age  vis_wuhan  from_wuhan  symptom1  \\\n",
            "0        104        8       1  66.0          1           0        14   \n",
            "1        101        8       0  56.0          0           1        14   \n",
            "2        137        8       1  46.0          0           1        14   \n",
            "3        116        8       0  60.0          1           0        14   \n",
            "4        116        8       1  58.0          0           0        14   \n",
            "5         23        8       0  44.0          0           1        14   \n",
            "6        105        8       1  34.0          0           1        14   \n",
            "7         13        8       1  37.0          1           0        14   \n",
            "8         13        8       1  39.0          1           0        14   \n",
            "9         13        8       1  56.0          1           0        14   \n",
            "10        13        8       0  18.0          1           0        14   \n",
            "\n",
            "    symptom2  symptom3  symptom4  symptom5  symptom6  diff_sym_hos  result  \n",
            "0         31        19        12         3         1             8       1  \n",
            "1         31        19        12         3         1             0       0  \n",
            "2         31        19        12         3         1            13       0  \n",
            "3         31        19        12         3         1             0       0  \n",
            "4         31        19        12         3         1             0       0  \n",
            "5         31        19        12         3         1             0       0  \n",
            "6         31        19        12         3         1             0       0  \n",
            "7         31        19        12         3         1             6       0  \n",
            "8         31        19        12         3         1             5       0  \n",
            "9         31        19        12         3         1             4       0  \n",
            "10        31        19        12         3         1             1       0  \n"
          ]
        }
      ],
      "source": [
        "names = ['location',\"country\",\"gender\",\"age\",\"vis_wuhan\",\"from_wuhan\",\"symptom1\",\n",
        "        \"symptom2\",\"symptom3\",\"symptom4\",\"symptom5\",\"symptom6\",\"diff_sym_hos\",\"result\"]\n",
        "df = pd.read_csv(\"data.csv\", header=None, skiprows=1, names=names)\n",
        "print(df.loc[0:10]) #visualising first 11 rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### divide the data into three partitions: training, validation, and testing\n",
        "#### used the conventional 70% training 15% validation and 15% testing parititioning with randomness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(863, 14)\n",
            "786    0\n",
            "190    0\n",
            "764    0\n",
            "52     0\n",
            "851    0\n",
            "      ..\n",
            "571    0\n",
            "173    0\n",
            "753    0\n",
            "419    1\n",
            "788    0\n",
            "Name: result, Length: 604, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "print(df.shape)\n",
        "X = df.drop(columns=['result'])  #dropping the column of the target\n",
        "Y = df['result']\n",
        "\n",
        "randomState=44\n",
        "train_ratio = 0.7\n",
        "validation_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Split the data using train_test_split with randomness\n",
        "XTrain, X_temp, YTrain, Y_temp = train_test_split(X, Y, test_size=1 - train_ratio, random_state=randomState)\n",
        "XValidation, XTest, YValidation, YTest = train_test_split(\n",
        "    X_temp, Y_temp, test_size=test_ratio / (validation_ratio + test_ratio), random_state=randomState)\n",
        "\n",
        "print(YTrain)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First Classification method is KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalizing,Fitting,Choosing Parameters \n",
        "\n",
        "1- I used StandardScaler for sklearn to normalize the data as you can see some of them are in 100s and some are just binary <br>\n",
        "2- I tried k from 1 to sqrt of the number of training data why? because it is a common heuristic and works well in practice <br>\n",
        "3- I used Validation data to choose the best K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best F1 Score 0.717948717948718\n",
            "The K with the best F1 Score 3\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "XTrain_scaled_ForKNN = scaler.fit_transform(XTrain)\n",
        "XValidation_scaled_ForKNN = scaler.transform(XValidation)\n",
        "xTest_scaled_ForKNN = scaler.transform(XTest)\n",
        "\n",
        "# Assuming X_train is your training data\n",
        "num_training_samples = len(XTrain_scaled_ForKNN)\n",
        "max_k = int(math.sqrt(num_training_samples))\n",
        "\n",
        "best_k = None\n",
        "best_f1 = 0.0\n",
        "for k in range (1,max_k+1,2):\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_classifier.fit(XTrain_scaled_ForKNN,YTrain)\n",
        "    y_pred_KNN = knn_classifier.predict(XValidation_scaled_ForKNN)\n",
        "\n",
        "    f1_KNN = f1_score(YValidation, y_pred_KNN)\n",
        "\n",
        "    if f1_KNN > best_f1:\n",
        "        best_f1 = f1_KNN\n",
        "        best_k = k\n",
        "print(\"Best F1 Score\",best_f1)\n",
        "print(\"The K with the best F1 Score\",best_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing\n",
        "Now, I will test using the test data using K=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score for testing 0.5000000000000001\n"
          ]
        }
      ],
      "source": [
        "y_test_KNN = knn_classifier.predict(xTest_scaled_ForKNN)\n",
        "f1_Test_KNN = f1_score(YTest, y_test_KNN)\n",
        "print(\"F1 Score for testing\",f1_Test_KNN)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

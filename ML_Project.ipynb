{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project Description\n",
        "The data used in this project will help to identify whether a person is going to recover from \n",
        "coronavirus symptoms or not based on some pre-defined standard symptoms. These symptoms are \n",
        "based on guidelines given by the World Health Organization (WHO).\n",
        "This dataset has daily level information on the number of affected cases, deaths and recovery from \n",
        "2019 novel coronavirus. Please note that this is a time series data and so the number of cases on \n",
        "any given day is the cumulative number.\n",
        "The data is available from 22 Jan, 2020. Data is in “data.csv”.\n",
        "The dataset contains 14 major variables that will be having an impact on whether someone has \n",
        "recovered or not, the description of each variable are as follows,\n",
        "1. Country: where the person resides\n",
        "2. Location: which part in the Country\n",
        "3. Age: Classification of the age group for each person, based on WHO Age Group Standard\n",
        "4. Gender: Male or Female \n",
        "5. Visited_Wuhan: whether the person has visited Wuhan, China or not\n",
        "6. From_Wuhan: whether the person is from Wuhan, China or not\n",
        "7. Symptoms: there are six families of symptoms that are coded in six fields.\n",
        "13. Time_before_symptoms_appear: \n",
        "14. Result: death (1) or recovered (0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### First we will import the main libraries for the whole Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load and Visualize the Data... since it is of a high dimensional we will just see the table of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    location  country  gender   age  vis_wuhan  from_wuhan  symptom1  \\\n",
            "0        104        8       1  66.0          1           0        14   \n",
            "1        101        8       0  56.0          0           1        14   \n",
            "2        137        8       1  46.0          0           1        14   \n",
            "3        116        8       0  60.0          1           0        14   \n",
            "4        116        8       1  58.0          0           0        14   \n",
            "5         23        8       0  44.0          0           1        14   \n",
            "6        105        8       1  34.0          0           1        14   \n",
            "7         13        8       1  37.0          1           0        14   \n",
            "8         13        8       1  39.0          1           0        14   \n",
            "9         13        8       1  56.0          1           0        14   \n",
            "10        13        8       0  18.0          1           0        14   \n",
            "\n",
            "    symptom2  symptom3  symptom4  symptom5  symptom6  diff_sym_hos  result  \n",
            "0         31        19        12         3         1             8       1  \n",
            "1         31        19        12         3         1             0       0  \n",
            "2         31        19        12         3         1            13       0  \n",
            "3         31        19        12         3         1             0       0  \n",
            "4         31        19        12         3         1             0       0  \n",
            "5         31        19        12         3         1             0       0  \n",
            "6         31        19        12         3         1             0       0  \n",
            "7         31        19        12         3         1             6       0  \n",
            "8         31        19        12         3         1             5       0  \n",
            "9         31        19        12         3         1             4       0  \n",
            "10        31        19        12         3         1             1       0  \n"
          ]
        }
      ],
      "source": [
        "names = ['location',\"country\",\"gender\",\"age\",\"vis_wuhan\",\"from_wuhan\",\"symptom1\",\n",
        "        \"symptom2\",\"symptom3\",\"symptom4\",\"symptom5\",\"symptom6\",\"diff_sym_hos\",\"result\"]\n",
        "df = pd.read_csv(\"data.csv\", header=None, skiprows=1, names=names)\n",
        "print(df.loc[0:10]) #visualising first 11 rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### divide the data into three partitions: training, validation, and testing\n",
        "#### used the conventional 70% training 15% validation and 15% testing parititioning with randomness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(863, 14)\n",
            "786    0\n",
            "190    0\n",
            "764    0\n",
            "52     0\n",
            "851    0\n",
            "      ..\n",
            "571    0\n",
            "173    0\n",
            "753    0\n",
            "419    1\n",
            "788    0\n",
            "Name: result, Length: 604, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "print(df.shape)\n",
        "X = df.drop(columns=['result'])  #dropping the column of the target\n",
        "Y = df['result']\n",
        "\n",
        "randomState=44\n",
        "train_ratio = 0.7\n",
        "validation_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Split the data using train_test_split with randomness\n",
        "XTrain, X_temp, YTrain, Y_temp = train_test_split(X, Y, test_size=1 - train_ratio, random_state=randomState)\n",
        "XValidation, XTest, YValidation, YTest = train_test_split(\n",
        "    X_temp, Y_temp, test_size=test_ratio / (validation_ratio + test_ratio), random_state=randomState)\n",
        "\n",
        "print(YTrain)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalizing Data\n",
        "I used StandardScaler for sklearn to normalize the data as you can see some of them are in 100s and some are just binary and some classifiers needs normalized data <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "XTrain_scaled = scaler.fit_transform(XTrain)\n",
        "XValidation_scaled = scaler.transform(XValidation)\n",
        "xTest_scaled = scaler.transform(XTest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First Classification method is KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fitting,Choosing Parameters \n",
        "\n",
        "1- I tried k from 1 to sqrt of the number of training data why? because it is a common heuristic and works well in practice <br>\n",
        "2- I used Validation data to choose the best K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best F1 Score 0.717948717948718\n",
            "The K with the best F1 Score 3\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import metrics\n",
        "\n",
        "# Assuming X_train is your training data\n",
        "num_training_samples = len(XTrain_scaled)\n",
        "max_k = int(math.sqrt(num_training_samples))\n",
        "\n",
        "best_k = None\n",
        "best_f1 = 0.0\n",
        "for k in range (1,max_k+1,2):\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_classifier.fit(XTrain_scaled,YTrain)\n",
        "    y_pred_KNN = knn_classifier.predict(XValidation_scaled)\n",
        "\n",
        "    f1_KNN = f1_score(YValidation, y_pred_KNN)\n",
        "\n",
        "    if f1_KNN > best_f1:\n",
        "        best_f1 = f1_KNN\n",
        "        best_k = k\n",
        "print(\"Best F1 Score\",best_f1)\n",
        "print(\"The K with the best F1 Score\",best_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing\n",
        "Now, I will test using the test data using K=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score for testing 0.6956521739130435\n"
          ]
        }
      ],
      "source": [
        "knn_classifier = KNeighborsClassifier(n_neighbors=best_k)\n",
        "knn_classifier.fit(XTrain_scaled,YTrain)\n",
        "\n",
        "y_test_KNN = knn_classifier.predict(xTest_scaled)\n",
        "f1_Test_KNN = f1_score(YTest, y_test_KNN)\n",
        "print(\"F1 Score for testing\",f1_Test_KNN)\n",
        "\n",
        "##14 0.6666 0.625\n",
        "# 15 0.8   0.4\n",
        "## 44 0.717   0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second Classification method Logistic Regression\n",
        "1-Importing Logistic Regression from scikit-learn <Br> \n",
        "2-Initializing Logistic Regression model <br> \n",
        "- Maximum of 1000 iterations as lower number of iterations gave me total number of iterations reached limit <br>\n",
        "- Solver='liblinear' because it works well with small and medium data for binary classes I tried others as well and the score was near <br>\n",
        "- Penalty='l2' as it works with this kind of data well and I tried 'l1' gave the same result<br>\n",
        "\n",
        "3-Getting the best F1 Score while trying different C <br>\n",
        "4-Fitting LR to the best C <br>\n",
        "5-Predicting and calculating F1 score on the testing data<br>\n",
        "\n",
        "I used scaled data as it has a very little impact on logistic Regression\n",
        "### finally some good F1 score!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best F1 Score 0.6875\n",
            "The C with the best F1 Score 1.0\n",
            "F1 score for testing Data 0.8181818181818182\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "best_C = None\n",
        "best_LR_f1 = 0.0\n",
        "for c in range(-1, 9):  \n",
        "    HyperparamC = 0.0001 * (10 ** c)\n",
        "    LR = LogisticRegression(C=HyperparamC,solver='liblinear',penalty='l2',max_iter=1000)\n",
        "    LR.fit(XTrain_scaled,YTrain)\n",
        "    y_pred_LR_Val = LR.predict(XValidation_scaled)\n",
        "\n",
        "    f1_LR = f1_score(YValidation, y_pred_LR_Val)\n",
        "\n",
        "    if f1_LR > best_LR_f1:\n",
        "        best_LR_f1 = f1_LR\n",
        "        best_C = HyperparamC\n",
        "print(\"Best F1 Score\",best_LR_f1)\n",
        "print(\"The C with the best F1 Score\",best_C)\n",
        "\n",
        "\n",
        "LR = LogisticRegression(C=best_C,solver='liblinear',max_iter=1000)\n",
        "LR.fit(XTrain_scaled,YTrain)\n",
        "\n",
        "\n",
        "\n",
        "y_pred_LR_Test = LR.predict(xTest_scaled)\n",
        "f1_LR_Test =f1_score(YTest, y_pred_LR_Test)\n",
        "print(\"F1 score for testing Data\", f1_LR_Test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Third Classification method Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1- Create a Gaussian Naïve Bayes classifier <br>\n",
        "2- Try to get the smoothing Value that gets the best f1 score<br>\n",
        "3- Make predictions on the testing data <br>\n",
        "4- Evaluate the F1 score of the classifier <br>\n",
        "<br>\n",
        "Note: I didn't normalize the data as it doesn't matter in Naïve Bayes <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best F1 Score on validation Data 0.5714285714285714\n",
            "The smoothing value with the best F1 Score 0.01\n",
            "F1 score for testing Data 0.7058823529411764\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "best_Naive_Smoothing_Value=0\n",
        "smoothing_values = np.logspace(-12, 0, 13,base=10)\n",
        "best_Naive_f1=0\n",
        "for smoothing_value in smoothing_values:\n",
        "    Naive = GaussianNB(var_smoothing=smoothing_value)\n",
        "    Naive.fit(XTrain, YTrain)\n",
        "    y_pred_Naive_Val = Naive.predict(XValidation)\n",
        "    f1_Naive_Val =f1_score(YValidation, y_pred_Naive_Val)\n",
        "    if f1_Naive_Val > best_Naive_f1:\n",
        "        best_Naive_f1 = f1_Naive_Val\n",
        "        best_Naive_Smoothing_Value = smoothing_value\n",
        "print(\"Best F1 Score on validation Data\",best_Naive_f1)\n",
        "print(\"The smoothing value with the best F1 Score\",best_Naive_Smoothing_Value)\n",
        "\n",
        "    \n",
        "Naive = GaussianNB(var_smoothing=best_Naive_Smoothing_Value)\n",
        "Naive.fit(XTrain, YTrain)\n",
        "\n",
        "y_Naive_pred = Naive.predict(XTest)\n",
        "f1_Naive_Test =f1_score(YTest, y_Naive_pred)\n",
        "print(\"F1 score for testing Data\", f1_Naive_Test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
